# ğŸ“„ PDF RAG Chatbot (Local LLM + FAISS + Streamlit)

A **Retrieval-Augmented Generation (RAG)** chatbot that answers questions from PDF documents using a **local LLM (TinyLlama via Ollama)**, **FAISS vector search**, and **SentenceTransformers embeddings**.

This project demonstrates an **end-to-end GenAI pipeline** with semantic search, context grounding, and low-memory optimization for local deployment.

---

## ğŸš€ Features

- ğŸ“‚ Upload any PDF and ask questions
- ğŸ” Semantic search using FAISS
- ğŸ§  Context-aware answers using RAG
- ğŸ¤– Local LLM inference with Ollama (TinyLlama)
- ğŸ’¬ Chat history in UI
- âš¡ Cached embeddings and index for performance
- ğŸ§® Low-RAM optimization (small chunks + limited context)

---

## ğŸ—ï¸ Architecture

User â†’ Streamlit UI â†’ PDF Loader â†’ Text Chunking â†’  
Embeddings (SentenceTransformers) â†’ FAISS Vector Search â†’  
Context Retrieval â†’ Ollama (TinyLlama) â†’ Answer

---

## ğŸ› ï¸ Tech Stack

- **LLM:** TinyLlama (via Ollama)
- **Embeddings:** sentence-transformers (all-MiniLM-L6-v2)
- **Vector DB:** FAISS
- **Frontend:** Streamlit
- **PDF Parsing:** pypdf
- **Language:** Python

---

## ğŸ“‚ Project Structure

pdf-rag-chatbot/
â”‚
â”œâ”€â”€ streamlit_app.py # Main Streamlit UI + RAG pipeline
â”œâ”€â”€ requirements.txt # Project dependencies
â”œâ”€â”€ .gitignore # Ignore cache, PDFs, FAISS index
â””â”€â”€ README.md # Project documentation




---

## âš™ï¸ Setup Instructions

'''md


### 1ï¸ Clone the Repository

```bash
git clone https://github.com/<your-username>/pdf-rag-chatbot.git
cd pdf-rag-chatbot


### 2 Create Virtual Envirenment

python -m venv venv
venv\Scripts\activate


### 3 Install Dependencies

pip install -r requirements.txt


### 4 Install and Run Ollama

ollama pull tinyllama

### 4 Run the Streamlit App

streamlit run pdf_chatbot_streamlit_app.py
